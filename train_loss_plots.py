train_loss = [68983.33846028645,
61876.193359375,
54621.00789388021,
47461.80358886719,
40576.27734375,
33865.37784830729,
27807.214803059895,
22204.07647705078,
17425.240641276043,
13306.768646240234,
9922.473856608072,
7332.964950561523,
5357.140787760417,
3950.491142272949,
2951.300710042318,
2291.0891316731772,
1853.1355819702148,
1567.0627670288086,
1376.9452514648438,
1263.0319112141926,
1191.1395924886067,
1150.5776952107747,
1131.956879933675,
1115.0193773905437,
1108.0229657491047,
1102.7426071166992,
1102.2481206258137,
1096.0454902648926,
1090.2503751118977,
1079.7950833638508,
1060.521557490031,
1038.580811182658,
1010.0500418345133,
957.6601861317953,
602.8615309397379,
285.8248914082845,
203.12039486567178,
174.05214230219522,
142.82562820116678,
119.24691637357076,
105.45018267631531,
91.21697807312012,
92.09003535906474,
86.82820351918538,
85.112029115359,
82.49785530567169,
80.46499490737915,
84.19789894421895,
78.08571155865987,
78.08914562066396,
79.1086489756902,
79.50612608591716,
74.90781716505687,
75.69055895010631,
78.8923287789027,
74.68664558728535,
79.96809697151184,
76.34892094135284,
76.2752947807312,
73.26406319936116,
72.22754800319672,
71.13241688410442,
67.3721155722936,
65.73799244562785,
56.07766898473104,
48.12033991018931,
45.86660212278366,
39.88007307052612,
36.44182046254476,
33.22781147559484,
29.952359199523926,
27.7762078444163,
26.356102367242176,
23.569219082593918,
23.94302962223689,
20.94836397965749,
20.50507577260335,
19.29802656173706]
valid_loss = [67061.17838541667,
59690.713216145836,
52628.09521484375,
45682.43798828125,
38767.260091145836,
32211.860107421875,
26166.761962890625,
20905.03662109375,
16196.456746419271,
12313.315063476562,
9160.790405273438,
6728.297688802083,
4945.167541503906,
3641.7100728352866,
2747.4806111653647,
2161.6668599446616,
1779.4134267171223,
1495.4558563232422,
1344.5713500976562,
1245.2606913248699,
1185.0871887207031,
1152.2928263346355,
1126.6304372151692,
1115.8797556559246,
1111.1843922932942,
1108.3704121907551,
1107.2479197184246,
1100.9176076253254,
1096.5085957845051,
1087.1701100667317,
1072.1033020019531,
1051.119519551595,
1038.8125534057617,
1036.724156697591,
472.93618138631183,
245.15673319498697,
216.33790334065756,
169.00983715057373,
141.19729646046957,
125.64271227518718,
115.85165055592854,
104.53101380666097,
112.95679378509521,
103.66738224029541,
105.60674254099528,
100.07362333933513,
106.14284610748291,
100.69776821136475,
101.05979379018147,
97.7715555826823,
114.14588737487793,
96.19496409098308,
96.76209195454915,
92.00191179911296,
88.75161139170329,
92.985902150472,
94.9124960899353,
88.18191083272298,
93.42009162902832,
91.47375424702962,
92.46252346038818,
84.68856398264568,
83.28713703155518,
82.837051709493,
72.09070984522502,
63.6772526105245,
71.10728581746419,
59.22399171193441,
51.4457434018453,
61.213577588399254,
52.8090877532959,
42.30221446355184,
42.66226609547933,
42.0902906258901,
38.79415440559387,
41.66307226816813,
39.21579233805338,
37.169575452804565]
epoch = [0,
2,
4,
6,
8,
10,
12,
14,
16,
18,
20,
22,
24,
26,
28,
30,
32,
34,
36,
38,
40,
42,
44,
46,
48,
50,
52,
54,
56,
58,
60,
62,
64,
66,
68,
70,
72,
74,
76,
78,
80,
82,
84,
86,
88,
90,
92,
94,
96,
98,
100,
102,
104,
106,
108,
110,
112,
114,
116,
118,
120,
122,
124,
126,
128,
130,
132,
134,
136,
138,
140,
142,
144,
146,
148,
150,
152,
154]

import matplotlib.pyplot as plt

plt.semilogy(epoch, train_loss, label="Train Loss")
plt.semilogy(epoch, valid_loss, label="Validation Loss")
plt.legend()
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss by Epoch")
plt.show()

plt.plot(epoch, train_loss, label="Train Loss")
plt.plot(epoch, valid_loss, label="Validation Loss")
plt.legend()
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss by Epoch")
plt.show()